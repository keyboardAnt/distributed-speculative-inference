{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class Target:\n",
    "    def __init__(self, model_name_or_path):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        self.pkv = None\n",
    "\n",
    "        # batch, head, seq, dim/head\n",
    "\n",
    "    def fix_pkv(self, start_id):\n",
    "        self.pkv = tuple(\n",
    "            (k[:, :, :start_id, :], v[:, :, :start_id, :]) for k, v in self.pkv\n",
    "        )\n",
    "\n",
    "    def verify(self, input_ids, start_id):\n",
    "        \"\"\"\n",
    "        start_id: absolute index\n",
    "        \"\"\"\n",
    "        if self.pkv is not None:\n",
    "            current_pkv_len = self.pkv[0][0].shape[2]\n",
    "\n",
    "            if start_id < current_pkv_len:\n",
    "                self.fix_pkv(start_id)\n",
    "            else:\n",
    "                start_id = current_pkv_len\n",
    "\n",
    "        input_ids_check = input_ids[:, start_id:]\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids_check, past_key_values=self.pkv, return_dict=True\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "\n",
    "        self.pkv = outputs.past_key_values\n",
    "\n",
    "        target_pred = logits.argmax(dim=-1)\n",
    "        target_pred = target_pred[:, :-1]\n",
    "        input_ids_check_continue = input_ids_check[:, 1:]\n",
    "\n",
    "        logging.error(f\"{input_ids_check_continue=}, {target_pred=}\")\n",
    "\n",
    "        draft_target_agree = input_ids_check != target_pred\n",
    "\n",
    "        logging.error(f\"{draft_target_agree=}\")\n",
    "\n",
    "        first_mistake_index_list = draft_target_agree.nonzero()\n",
    "\n",
    "        if len(first_mistake_index_list) == 0:\n",
    "            # no mistakes\n",
    "            mistake_index = None\n",
    "            correct_token = None\n",
    "        else:\n",
    "            logging.error(f\"{first_mistake_index_list=}\")\n",
    "            mistake_index = first_mistake_index_list[0][1]\n",
    "            correct_token = target_pred[0, mistake_index]\n",
    "        logging.error(f\"{correct_token=}, {mistake_index=}\")\n",
    "\n",
    "        return correct_token, mistake_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Target(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Danny has a hat and a suit. What does Danny have? Danny has a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = target.model.generate(**inps, do_sample=False, max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ids = tokenizer.convert_ids_to_tokens(outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ids = outs[:, :22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ids = tensor(\n",
    "    [\n",
    "        [\n",
    "            45478,\n",
    "            468,\n",
    "            257,\n",
    "            6877,\n",
    "            290,\n",
    "            257,\n",
    "            6050,\n",
    "            13,\n",
    "            1867,\n",
    "            857,\n",
    "            15105,\n",
    "            423,\n",
    "            30,\n",
    "            15105,\n",
    "            468,\n",
    "            257,\n",
    "            6877,\n",
    "            290,\n",
    "            257,\n",
    "            1219,\n",
    "            1113,\n",
    "            1919,\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_ids == 1219).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.pkv = None\n",
    "res = target.verify(new_ids, inps[\"input_ids\"].shape[1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    (\"Danny\", tensor(45478)),\n",
    "    (\"Ġhas\", tensor(468)),\n",
    "    (\"Ġa\", tensor(257)),\n",
    "    (\"Ġhat\", tensor(6877)),\n",
    "    (\"Ġand\", tensor(290)),\n",
    "    (\"Ġa\", tensor(257)),\n",
    "    (\"Ġsuit\", tensor(6050)),\n",
    "    (\".\", tensor(13)),\n",
    "    (\"ĠWhat\", tensor(1867)),\n",
    "    (\"Ġdoes\", tensor(857)),\n",
    "    (\"ĠDanny\", tensor(15105)),\n",
    "    (\"Ġhave\", tensor(423)),\n",
    "    (\"?\", tensor(30)),\n",
    "    (\"ĠDanny\", tensor(15105)),\n",
    "    (\"Ġhas\", tensor(468)),\n",
    "    (\"Ġa\", tensor(257)),\n",
    "    (\"Ġhat\", tensor(6877)),\n",
    "    (\"Ġand\", tensor(290)),\n",
    "    (\"Ġa\", tensor(257)),\n",
    "    (\"Ġsuit\", tensor(12893)),\n",
    "    (\".\", tensor(13)),\n",
    "    (\"ĠWhat\", tensor(1867)),\n",
    "    (\"Ġdoes\", tensor(857)),\n",
    "    (\"ĠDanny\", tensor(15105)),\n",
    "    (\"Ġhave\", tensor(423)),\n",
    "    (\"?\", tensor(30)),\n",
    "    (\"Ċ\", tensor(198)),\n",
    "    (\"Ċ\", tensor(198)),\n",
    "    (\"The\", tensor(464)),\n",
    "    (\"Ġfirst\", tensor(717)),\n",
    "    (\"Ġtime\", tensor(640)),\n",
    "    (\"ĠI\", tensor(314)),\n",
    "    (\"Ġsaw\", tensor(2497)),\n",
    "    (\"Ġhim\", tensor(683)),\n",
    "    (\",\", tensor(11)),\n",
    "    (\"ĠI\", tensor(314)),\n",
    "    (\"Ġwas\", tensor(373)),\n",
    "    (\"Ġlike\", tensor(588)),\n",
    "    (\",\", tensor(11)),\n",
    "    ('Ġ\"', tensor(366)),\n",
    "    (\"Oh\", tensor(5812)),\n",
    "    (\",\", tensor(11)),\n",
    "    (\"Ġhe\", tensor(339)),\n",
    "    (\"'s\", tensor(338)),\n",
    "    (\"Ġa\", tensor(257)),\n",
    "    (\"Ġguy\", tensor(3516)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(out_ids, outs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([2, 31414, 6, 127, 766, 16, 344, 4, 347, 4, 8, 38, 524, 10, 1294, 23, 5, 589, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch.multiprocessing import Process, Queue\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tok_ids = torch.tensor(\n",
    "    [[15205, 541, 305, 919, 278, 351, 12905, 2667, 15399, 714, 307, 281, 220]]\n",
    ")\n",
    "\n",
    "\n",
    "def fwd(model, tok_ids, queue):\n",
    "    print(\"Starting process\")\n",
    "    print(f\"{os.environ['TOKENIZERS_PARALLELISM']=}\")\n",
    "    print(f\"{type(model)=}\")\n",
    "    print(f\"{tok_ids=}\")\n",
    "    try:\n",
    "        outs = model(tok_ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print(f\"{outs=}\")\n",
    "    queue.put(outs)\n",
    "\n",
    "\n",
    "queue = Queue()\n",
    "pr = Process(target=fwd, args=(model, tok_ids, queue))\n",
    "pr.start()\n",
    "pr.join()\n",
    "outs = queue.get()\n",
    "print(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workaround with async-await"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of all the prefixes of `tok_ids`\n",
    "all_tok_ids = [tok_ids[:, :i] for i in range(1, tok_ids.shape[1] + 1)]\n",
    "all_tok_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def fwd(model, tok_ids):\n",
    "    print(\"Starting process\")\n",
    "    print(f\"{os.environ['TOKENIZERS_PARALLELISM']=}\")\n",
    "    print(f\"{type(model)=}\")\n",
    "    print(f\"{tok_ids=}\")\n",
    "    try:\n",
    "        outs = model(tok_ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print(f\"{outs=}\")\n",
    "    return outs\n",
    "\n",
    "\n",
    "async def main():\n",
    "    print(\"Starting main\")\n",
    "    tasks = [fwd(model, t) for t in all_tok_ids]\n",
    "    print(\"Running tasks\")\n",
    "    outs = await asyncio.gather(*tasks)\n",
    "    print(outs)\n",
    "\n",
    "\n",
    "# asyncio.run(main())\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

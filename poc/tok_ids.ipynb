{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting garbage...\n",
      "'The meetings can be live or virtual, but with the pandemic continuing in many parts of the world, many companies will opt for virtual meetings in order to minimize the spread of illness. Virtual meetings also bring an array of advantages like being able to connect with people in different parts of the world. \\n\\n'\n"
     ]
    }
   ],
   "source": [
    "from dsi import encode, decode\n",
    "import torch\n",
    "\n",
    "def print_decoded(tok_ids, model_name):\n",
    "    out_str = \"\".join(decode(tok_ids, model_name))\n",
    "    print(repr(out_str))\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "accepted_tok_ids = torch.tensor([  791, 16659,   649,   387,  3974,   477,  4200,    11,   719,   449,\n",
    "          279, 28522, 14691,   304,  1690,  5596,   315,   279,  1917,    11,\n",
    "         1690,  5220,   690,  3469,   369,  4200, 16659,   304,  2015,   311,\n",
    "        30437,   279,  9041,   315, 17563,    13, 21382, 16659,  1101,  4546,\n",
    "          459,  1358,   315, 22934,  1093,  1694,  3025,   311,  4667,   449,\n",
    "         1274,   304,  2204,  5596,   315,   279,  1917,    13,  4815])\n",
    "print_decoded(accepted_tok_ids, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting garbage...\n",
      "'The meetings can be live or virtual, but with the pandemic continuing in many parts of the world, many companies will opt for virtual meetings in order to minimize the spread of illness. Virtual meetings also bring an array of advantages like being able to connect with people in different parts of the world. \\n'\n"
     ]
    }
   ],
   "source": [
    "draft_tok_ids = torch.tensor([  791, 16659,   649,   387,  3974,   477,  4200,    11,   719,   449,\n",
    "          279, 28522, 14691,   304,  1690,  5596,   315,   279,  1917,    11,\n",
    "         1690,  5220,   690,  3469,   369,  4200, 16659,   304,  2015,   311,\n",
    "        30437,   279,  9041,   315, 17563,    13, 21382, 16659,  1101,  4546,\n",
    "          459,  1358,   315, 22934,  1093,  1694,  3025,   311,  4667,   449,\n",
    "         1274,   304,  2204,  5596,   315,   279,  1917,    13,   720])\n",
    "print_decoded(draft_tok_ids, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting garbage...\n",
      "' \\n\\n'\n",
      "Collecting garbage...\n",
      "' \\n'\n"
     ]
    }
   ],
   "source": [
    "print_decoded(torch.tensor([4815]), model_name)\n",
    "print_decoded(torch.tensor([720]), model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distributed-speculative-inference-w7-NjH3e-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

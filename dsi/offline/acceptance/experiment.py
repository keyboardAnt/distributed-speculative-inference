import logging

import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

from dsi.configs.experiment.acceptance import ConfigAcceptanteRate
from dsi.configs.experiment.generation import ConfigGen
from dsi.online.latency.experiment import ExperimentLatency
from dsi.types.result import ResultAcceptance

log = logging.getLogger(__name__)


class ExperimentAcceptanceRate(ExperimentLatency):
    """
    Measures the generation acceptance rate.
    """

    def __init__(
        self,
        config: ConfigAcceptanteRate,
        gen_config: ConfigGen,
        draft_gen_config: ConfigGen,
    ):
        self.config: ConfigAcceptanteRate
        super().__init__(config, gen_config)
        self.draft_gen_config: ConfigGen = draft_gen_config

    def _load_draft_model_tokenizer(self) -> tuple:
        log.info(
            f"Loading model: {self.config.draft_model}, \
            compile={self.config.draft_compile_model}"
        )
        model = AutoModelForCausalLM.from_pretrained(
            self.config.draft_model,
            device_map="auto",
            torch_dtype=self.config.get_torch_draft_dtype(),
            revision=self.config.draft_revision,
        )
        tokenizer = AutoTokenizer.from_pretrained(self.config.draft_model)
        model = torch.compile(model) if self.config.draft_compile_model else model
        return model, tokenizer

    def _are_tokenizers_same(self, tokenizer1, tokenizer2) -> bool:
        # Compare vocabularies
        if tokenizer1.get_vocab() != tokenizer2.get_vocab():
            return False

        # Compare special tokens
        special_tokens_to_compare = [
            "eos_token_id",
            "pad_token_id",
            "bos_token_id",
            "unk_token_id",
        ]
        for token in special_tokens_to_compare:
            if getattr(tokenizer1, token, None) != getattr(tokenizer2, token, None):
                return False

        return True

    def _single_repeat(self) -> ResultAcceptance:
        all_n_matches = []

        examples = self._get_random_prompted_examples()

        target_model, target_tokenizer = self._load_model_tokenizer()
        draft_model, draft_tokenizer = self._load_draft_model_tokenizer()

        # Check if tokenizers are the same
        if not self._are_tokenizers_same(target_tokenizer, draft_tokenizer):
            raise ValueError("The target and draft tokenizers are not the same.")

        target_gen_kwargs = dict(
            do_sample=self.gen_config.do_sample,
            temperature=self.gen_config.temperature,
            top_p=self.gen_config.top_p,
            pad_token_id=target_tokenizer.eos_token_id,
            max_new_tokens=self.config.max_new_tokens,
        )

        draft_gen_kwargs = dict(
            do_sample=self.draft_gen_config.do_sample,
            temperature=self.draft_gen_config.temperature,
            top_p=self.draft_gen_config.top_p,
            pad_token_id=target_tokenizer.eos_token_id,
            max_new_tokens=1,
        )

        for ex in tqdm(examples):
            inputs = target_tokenizer(ex, return_tensors="pt").to(target_model.device)
            n_matches = [0]
            output_target = target_model.generate(**inputs, **target_gen_kwargs)
            prompt_len = len(inputs.input_ids[0])

            # iterate over the tokens generated by the target and
            # check whether the draft produces the same token
            for i in range(prompt_len, len(output_target[0])):
                inputs["input_ids"] = output_target[0, 0:i].view(1, -1)
                inputs["attention_mask"] = torch.tensor(
                    [[1] * i], device=draft_model.device
                )
                output_draft = draft_model.generate(**inputs, **draft_gen_kwargs)
                if output_draft[-1, i] == output_target[-1, i]:
                    n_matches[-1] += 1
                elif i < len(output_target[0]) - 1:  # new window
                    n_matches.append(0)
                else:  # at the end, remove last window
                    n_matches.pop()
            all_n_matches += n_matches
        ar = 1 - (1 / (1 + np.array(all_n_matches).mean()))
        return ResultAcceptance(acceptance_rate=[ar])
